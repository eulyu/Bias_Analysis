{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc573ac7",
   "metadata": {},
   "source": [
    "### Download PDB structures corresponding to GPCR Uniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "IN_CSV = Path(\"GPCR_Human_PDB_info.csv\")\n",
    "OUT_CSV = Path(\"GPCR_Uniprot2PDB.csv\")\n",
    "\n",
    "# UniProt Accession column\n",
    "ACC_COL = \"Entry\"  \n",
    "\n",
    "SEARCH_URL = \"https://search.rcsb.org/rcsbsearch/v2/query\"\n",
    "CORE_ENTRY_URL = \"https://data.rcsb.org/rest/v1/core/entry/{}\"\n",
    "\n",
    "SLEEP_SEC = 0.05\n",
    "RETRIES = 5\n",
    "TIMEOUT = 60\n",
    "\n",
    "def _sleep_backoff(attempt):\n",
    "    time.sleep((2 ** attempt) * 0.5)\n",
    "\n",
    "def post_json(session, url, payload, retries=RETRIES, timeout=TIMEOUT):\n",
    "    last_err = None\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            r = session.post(url, json=payload, timeout=timeout)\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                _sleep_backoff(attempt)\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            _sleep_backoff(attempt)\n",
    "    raise last_err\n",
    "\n",
    "def get_json(session, url, retries=RETRIES, timeout=TIMEOUT):\n",
    "    last_err = None\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            r = session.get(url, timeout=timeout)\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                _sleep_backoff(attempt)\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            _sleep_backoff(attempt)\n",
    "    raise last_err\n",
    "\n",
    "def rcsb_pdb_ids_by_uniprot(session, accession):\n",
    "    acc = str(accession).strip().upper()\n",
    "    if not acc or acc == \"NAN\":\n",
    "        return []\n",
    "\n",
    "    payload = {\n",
    "        \"query\": {\n",
    "            \"type\": \"group\",\n",
    "            \"logical_operator\": \"and\",\n",
    "            \"nodes\": [\n",
    "                {\n",
    "                    \"type\": \"terminal\",\n",
    "                    \"service\": \"text\",\n",
    "                    \"parameters\": {\n",
    "                        \"attribute\": \"rcsb_polymer_entity_container_identifiers.reference_sequence_identifiers.database_name\",\n",
    "                        \"operator\": \"exact_match\",\n",
    "                        \"value\": \"UniProt\",\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"terminal\",\n",
    "                    \"service\": \"text\",\n",
    "                    \"parameters\": {\n",
    "                        \"attribute\": \"rcsb_polymer_entity_container_identifiers.reference_sequence_identifiers.database_accession\",\n",
    "                        \"operator\": \"exact_match\",\n",
    "                        \"value\": acc,\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"return_type\": \"entry\",\n",
    "        \"request_options\": {\n",
    "            \"return_all_hits\": True,\n",
    "            \"results_content_type\": [\"experimental\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    j = post_json(session, SEARCH_URL, payload)\n",
    "    hits = j.get(\"result_set\", []) or []\n",
    "    return sorted({h.get(\"identifier\") for h in hits if h.get(\"identifier\")})\n",
    "\n",
    "def extract_pdbx_database_related_details(entry_json):\n",
    "    items = entry_json.get(\"pdbx_database_related\", []) or []\n",
    "    details_list = []\n",
    "    for it in items:\n",
    "        if isinstance(it, dict):\n",
    "            d = it.get(\"details\")\n",
    "            if d:\n",
    "                details_list.append(str(d).strip())\n",
    "    return \" | \".join(details_list) if details_list else None\n",
    "\n",
    "\n",
    "df = pd.read_csv(IN_CSV, dtype={ACC_COL: \"string\"})\n",
    "\n",
    "accessions = df[ACC_COL].dropna().astype(str).str.strip().str.upper()\n",
    "accessions = accessions[accessions.ne(\"\") & accessions.ne(\"NAN\")].unique().tolist()\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"GPCRbias/rcsb-search+core\"})\n",
    "\n",
    "acc_cache = {}\n",
    "pdb_details_cache = {}\n",
    "\n",
    "rows = []\n",
    "for acc in tqdm(accessions, desc=\"UniProt -> PDB\", unit=\"acc\"):\n",
    "    if acc in acc_cache:\n",
    "        pdbs = acc_cache[acc]\n",
    "    else:\n",
    "        try:\n",
    "            pdbs = rcsb_pdb_ids_by_uniprot(session, acc)\n",
    "        except Exception:\n",
    "            pdbs = []\n",
    "        acc_cache[acc] = pdbs\n",
    "        time.sleep(SLEEP_SEC)\n",
    "\n",
    "    for pdb in pdbs:\n",
    "        if pdb in pdb_details_cache:\n",
    "            details = pdb_details_cache[pdb]\n",
    "        else:\n",
    "            try:\n",
    "                entry = get_json(session, CORE_ENTRY_URL.format(pdb))\n",
    "                details = extract_pdbx_database_related_details(entry)\n",
    "            except Exception:\n",
    "                details = None\n",
    "            pdb_details_cache[pdb] = details\n",
    "            time.sleep(SLEEP_SEC)\n",
    "\n",
    "        rows.append({\n",
    "            \"UniprotID\": acc,\n",
    "            \"PDB\": pdb,\n",
    "            \"details\": details,\n",
    "        })\n",
    "\n",
    "out_df = pd.DataFrame(rows).drop_duplicates().reset_index(drop=True)\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"saved:\", OUT_CSV)\n",
    "print(\"row:\", len(out_df))\n",
    "print(\"UniprotID unique number:\", out_df[\"UniprotID\"].nunique())\n",
    "print(\"PDB unique number:\", out_df[\"PDB\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06adad44",
   "metadata": {},
   "source": [
    "### Extracting macromolecule entities from PDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d9fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjw/anaconda3/envs/hda2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PDB -> polymer entities: 100%|██████████| 2199/2199 [38:59<00:00,  1.06s/pdb] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "IN_CSV = Path(\"GPCR_PDB2Uniprot.csv\")\n",
    "OUT_ENTITIES = Path(\"GPCR_PDB_macromolecules_entities.csv\")\n",
    "BASE_URL = \"https://data.rcsb.org/rest/v1/core\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(IN_CSV)\n",
    "pdbs = df[\"PDB\"].dropna().astype(str).str.strip().str.upper().unique()\n",
    "\n",
    "session = requests.Session()\n",
    "rows = []\n",
    "\n",
    "for pdb in tqdm(pdbs, desc=\"Fetching Metadata\"):\n",
    "    entry_data = session.get(f\"{BASE_URL}/entry/{pdb}\").json()\n",
    "    ent_ids = entry_data.get(\"rcsb_entry_container_identifiers\", {}).get(\"polymer_entity_ids\", [])\n",
    "\n",
    "    for ent_id in ent_ids:\n",
    "        ent_data = session.get(f\"{BASE_URL}/polymer_entity/{pdb}/{ent_id}\").json()\n",
    "\n",
    "        desc = ent_data.get(\"rcsb_polymer_entity\", {}).get(\"pdbx_description\", \"\")\n",
    "        chain = ent_data.get(\"entity_poly\", {}).get(\"pdbx_strand_id\", \"\")\n",
    "        \n",
    "        refs = ent_data.get(\"rcsb_polymer_entity_container_identifiers\", {}).get(\"reference_sequence_identifiers\", [])\n",
    "        uniprots = sorted(set([\n",
    "            r[\"database_accession\"] for r in refs \n",
    "            if r.get(\"database_name\") == \"UniProt\" and \"database_accession\" in r\n",
    "        ]))\n",
    "\n",
    "        rows.append({\n",
    "            \"PDB\": pdb,\n",
    "            \"polymer_entity_id\": ent_id,\n",
    "            \"chain_id\": chain,\n",
    "            \"macromolecule_name\": desc,\n",
    "            \"entity_uniprot_accessions\": \";\".join(uniprots)\n",
    "        })\n",
    "\n",
    "entities_df = pd.DataFrame(rows)\n",
    "base_df = df[[\"PDB\", \"UniprotID\"]].drop_duplicates()\n",
    "base_df[\"PDB\"] = base_df[\"PDB\"].str.upper().str.strip()\n",
    "\n",
    "final_df = entities_df.merge(base_df, on=\"PDB\", how=\"left\")\n",
    "final_df.to_csv(OUT_ENTITIES, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322c680",
   "metadata": {},
   "source": [
    "### Classification of PDB Structures Based on G-protein and Beta-arrestin Macromolecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e6171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/kjw/GPCRbias/Data/GPCR/Group/GPCR_PDB_group.csv rows: 1200\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "IN_ENTITIES = Path(\"GPCR_PDB_macromolecules_entities.csv\")\n",
    "OUT_CSV = Path(\"Group/GPCR_PDB_group.csv\")\n",
    "\n",
    "df = pd.read_csv(IN_ENTITIES)\n",
    "\n",
    "\n",
    "df[\"PDB\"] = df[\"PDB\"].astype(str).str.strip().str.upper()\n",
    "df[\"UniprotID\"] = df[\"UniprotID\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "## Multiple GPCR complex PDB exception\n",
    "multi_pdbs = set(\n",
    "    df[[\"PDB\",\"UniprotID\"]].dropna().drop_duplicates()\n",
    "      .groupby(\"PDB\")[\"UniprotID\"].nunique()\n",
    "      .loc[lambda s: s >= 2].index\n",
    ")\n",
    "\n",
    "df = df.loc[~df[\"PDB\"].isin(multi_pdbs)].copy()\n",
    "df[\"chain_id\"] = df.get(\"chain_id\", \"\").fillna(\"\").astype(str).str.strip()\n",
    "df[\"macromolecule_name\"] = df[\"macromolecule_name\"].astype(str).str.replace(r\"[‐-‒–—−]\", \"-\", regex=True)\n",
    "\n",
    "blob = df[\"macromolecule_name\"].str.lower()\n",
    "\n",
    "gprot_patterns = [\n",
    "    r\"\\bg protein\\b\", r\"\\bheterotrimer\", r\"\\bgi\\b\", r\"\\bgs\\b\", r\"\\bgq\\b\", r\"\\bgo\\b\",\n",
    "    r\"\\bmini[- ]?g\\b\", r\"\\bminig\\b\",\n",
    "    r\"\\bgnai\", r\"\\bgnas\", r\"\\bgnaq\", r\"\\bgnao\", r\"\\bgnb\", r\"\\bgng\",\n",
    "    r\"\\bg alpha\\b\", r\"\\bg beta\\b\", r\"\\bg gamma\\b\",\n",
    "    r\"\\bguanine nucleotide[- ]binding protein\\b\",\n",
    "]\n",
    "arr_patterns = [\n",
    "    r\"\\bbeta[\\s\\-_]*arrestin(?:[\\s\\-_]*[12])?\\b\",\n",
    "    r\"\\bβ[\\s\\-_]*arrestin(?:[\\s\\-_]*[12])?\\b\",\n",
    "    r\"\\barrb[\\s\\-_]*1\\b\", r\"\\barrb[\\s\\-_]*2\\b\",\n",
    "    r\"\\barrestin[\\s\\-_]*2\\b\", r\"\\barrestin[\\s\\-_]*3\\b\",\n",
    "    r\"\\bbeta[\\s\\-_]*arrestin.*scfv\",\n",
    "]\n",
    "exclude_arr_patterns = [\n",
    "    r\"\\bs[\\s\\-_]*arrestin\\b\",\n",
    "    r\"\\bvisual[\\s\\-_]*arrestin\\b\",\n",
    "    r\"\\bsag\\b\",\n",
    "    r\"\\bp10523\\b\",   # SAG(visual arrestin) UniProt\n",
    "]\n",
    "\n",
    "g_re = re.compile(\"|\".join(gprot_patterns))\n",
    "ex_re = re.compile(\"|\".join(exclude_arr_patterns), flags=re.IGNORECASE)\n",
    "a_re  = re.compile(\"|\".join(arr_patterns), flags=re.IGNORECASE)\n",
    "\n",
    "df[\"_is_g_\"] = blob.apply(lambda s: bool(g_re.search(s)))\n",
    "df[\"_is_a_\"] = blob.apply(lambda s: (not ex_re.search(s)) and bool(a_re.search(s)))\n",
    "\n",
    "def uniq_join(x):\n",
    "    vals = []\n",
    "    seen = set()\n",
    "    for v in x:\n",
    "        v = str(v).strip()\n",
    "        if v and v not in seen:\n",
    "            seen.add(v)\n",
    "            vals.append(v)\n",
    "    return \";\".join(vals)\n",
    "\n",
    "g_df = df[df[\"_is_g_\"]].copy()\n",
    "a_df = df[df[\"_is_a_\"]].copy()\n",
    "\n",
    "g_agg = (\n",
    "    g_df.groupby(\"PDB\", as_index=False)\n",
    "        .agg(gprotein_chain_ids=(\"chain_id\", uniq_join),\n",
    "             gprotein_names=(\"macromolecule_name\", uniq_join))\n",
    ")\n",
    "\n",
    "a_agg = (\n",
    "    a_df.groupby(\"PDB\", as_index=False)\n",
    "        .agg(betaarrestin_chain_ids=(\"chain_id\", uniq_join),\n",
    "             betaarrestin_names=(\"macromolecule_name\", uniq_join))\n",
    ")\n",
    "\n",
    "pdb_to_uniprot = (\n",
    "    df[[\"PDB\", \"UniprotID\"]]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .groupby(\"PDB\")[\"UniprotID\"]\n",
    "    .apply(lambda s: \";\".join(pd.Series(s).astype(str).str.strip().replace(\"\", pd.NA).dropna().drop_duplicates()))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "all_pdb = (\n",
    "    pd.DataFrame({\"PDB\": df[\"PDB\"].dropna().drop_duplicates().sort_values()})\n",
    "    .merge(pdb_to_uniprot, on=\"PDB\", how=\"left\")\n",
    ")\n",
    "\n",
    "out = all_pdb.merge(g_agg, on=\"PDB\", how=\"left\").merge(a_agg, on=\"PDB\", how=\"left\")\n",
    "\n",
    "def classify(row):\n",
    "    g = isinstance(row.get(\"gprotein_chain_ids\"), str) and row[\"gprotein_chain_ids\"].strip() != \"\"\n",
    "    a = isinstance(row.get(\"betaarrestin_chain_ids\"), str) and row[\"betaarrestin_chain_ids\"].strip() != \"\"\n",
    "    if g and not a:\n",
    "        return \"G-protein\"\n",
    "    if a and not g:\n",
    "        return \"Beta-arrestin\"\n",
    "\n",
    "out[\"Group\"] = out.apply(classify, axis=1)\n",
    "missing_pdbs = out.loc[out[\"Group\"].isna() | (out[\"Group\"].astype(str).str.strip() == \"\"), \"PDB\"]\n",
    "\n",
    "if not missing_pdbs.empty:\n",
    "    missing_df = df[df[\"PDB\"].isin(missing_pdbs)]\n",
    "    MISSING_CSV = Path(\"Group/GPCR_PDB_unclassified_entities.csv\")\n",
    "    missing_df.to_csv(MISSING_CSV, index=False)\n",
    "    print(f\"Saved (PDBs: {len(missing_pdbs)})\")\n",
    "\n",
    "out = out[out[\"Group\"].notna() & (out[\"Group\"].astype(str).str.strip() != \"\")]\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(OUT_CSV, index=False)\n",
    "print(\"saved:\", OUT_CSV, \"rows:\", len(out))\n",
    "print(out['Group'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91eade5",
   "metadata": {},
   "source": [
    "### Sequence alignment and PDB chain extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from Bio.PDB import MMCIFParser\n",
    "from Bio import pairwise2\n",
    "from Bio.Align import substitution_matrices\n",
    "from tqdm import tqdm\n",
    "\n",
    "IN_CSV = \"Group/GPCR_PDB_group.csv\"\n",
    "OUT_FILE = 'Group/GPCR_PDB_chain.csv'\n",
    "FASTA_DIR = \"uniprot_fasta\"\n",
    "CIF_DIR = \"pdb_cif\"\n",
    "\n",
    "\n",
    "os.makedirs(CIF_DIR, exist_ok=True)\n",
    "os.makedirs(FASTA_DIR, exist_ok=True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "AA_MAP = {\n",
    "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
    "    'GLN': 'Q', 'GLU': 'E', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
    "    'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
    "    'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V',\n",
    "    'MSE': 'M', 'PTR': 'Y', 'TPO': 'T', 'SEP': 'S'\n",
    "}\n",
    "\n",
    "def to_1letter(resname):\n",
    "    return AA_MAP.get(resname, 'X')\n",
    "\n",
    "def download_cif(pdb_id):\n",
    "    path = os.path.join(CIF_DIR, f\"{pdb_id}.cif\")\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 1000:\n",
    "        return path\n",
    "    \n",
    "    url = f\"https://files.rcsb.org/download/{pdb_id}.cif\"\n",
    "    r = requests.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return path\n",
    "\n",
    "def fetch_uniprot_seq(uniprot_id):\n",
    "    if not uniprot_id or pd.isna(uniprot_id): return None\n",
    "    \n",
    "    path = os.path.join(FASTA_DIR, f\"{uniprot_id}.fasta\")\n",
    "    \n",
    "    if not (os.path.exists(path) and os.path.getsize(path) > 0):\n",
    "        url = f\"https://www.uniprot.org/uniprot/{uniprot_id}.fasta\"\n",
    "        r = requests.get(url, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(r.text)\n",
    "            \n",
    "    # 파일 읽어서 서열 반환 (헤더 제외)\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    return \"\".join([line.strip() for line in lines if not line.startswith(\">\")])\n",
    "\n",
    "def extract_chain_seq(structure, chain_id):\n",
    "    if chain_id not in structure[0]: return None\n",
    "    chain = structure[0][chain_id]\n",
    "    residues = [res for res in chain.get_residues() if 'CA' in res]\n",
    "    seq = \"\"\n",
    "    for res in residues:\n",
    "        seq += to_1letter(res.get_resname())\n",
    "    return seq\n",
    "\n",
    "def compute_alignment(uniprot_seq, pdb_seq, matrix):\n",
    "    if not uniprot_seq or not pdb_seq: return None\n",
    "\n",
    "    alignments = pairwise2.align.localds(uniprot_seq, pdb_seq, matrix, -10, -0.5)\n",
    "    \n",
    "    if not alignments: return None\n",
    "\n",
    "    best = alignments[0]\n",
    "    seqA, seqB = best.seqA, best.seqB\n",
    "    \n",
    "    matches = sum(1 for a, b in zip(seqA, seqB) if a == b and a != '-')\n",
    "    aligned_len = sum(1 for a, b in zip(seqA, seqB) if a != '-' or b != '-')\n",
    "    is_pass = (matches / len(uniprot_seq) >= 0.7) & (matches / aligned_len >= 0.7) & (matches / len(pdb_seq) >= 1.0)\n",
    "        \n",
    "\n",
    "    return {\n",
    "        'score': best.score,\n",
    "        'identity': matches / aligned_len if aligned_len else 0,\n",
    "        'coverageUniProt': matches / len(uniprot_seq),\n",
    "        'coverageChain': matches / len(pdb_seq),\n",
    "        'chain_length': len(pdb_seq),\n",
    "        'Pass' : is_pass\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(IN_CSV)\n",
    "    unique_pairs = df[['PDB', 'UniprotID']].dropna().drop_duplicates()\n",
    "    \n",
    "    parser = MMCIFParser(QUIET=True)\n",
    "    \n",
    "    blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
    "    \n",
    "    results = []\n",
    "    pbar = tqdm(unique_pairs.iterrows(), total=unique_pairs.shape[0], desc=\"Starting\")\n",
    "    \n",
    "    for i, (_, row) in enumerate(unique_pairs.iterrows()):\n",
    "        pdb_id = row['PDB'].strip().upper()\n",
    "        uniprot_id = str(row['UniprotID']).strip()\n",
    "        \n",
    "        cif_path = download_cif(pdb_id)\n",
    "        if not cif_path: continue\n",
    "\n",
    "        uni_seq = fetch_uniprot_seq(uniprot_id)\n",
    "        if not uni_seq: continue\n",
    "\n",
    "        try:\n",
    "            structure = parser.get_structure(pdb_id, cif_path)\n",
    "        except Exception:\n",
    "            continue\n",
    "        \n",
    "        for chain in structure[0]:\n",
    "            chain_id = chain.id\n",
    "            pdb_seq = extract_chain_seq(structure, chain_id)\n",
    "            \n",
    "            if not pdb_seq or len(pdb_seq) < 10: continue\n",
    "            \n",
    "            metrics = compute_alignment(uni_seq, pdb_seq, blosum62)\n",
    "            if metrics and metrics['score'] > 0:\n",
    "                results.append({\n",
    "                    'PDB': pdb_id,\n",
    "                    'UniprotID': uniprot_id,\n",
    "                    'chain_id': chain_id,\n",
    "                    **metrics\n",
    "                })\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"{i + 1}/{len(unique_pairs)}\")\n",
    "\n",
    "    res_df = pd.DataFrame(results)\n",
    "    \n",
    "    final_df = pd.merge(df, res_df, on=['PDB', 'UniprotID'], how='right')\n",
    "    \n",
    "    final_df.to_csv(OUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36713e05",
   "metadata": {},
   "source": [
    "### Group Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aebc4711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Total_PDBs  Passed_PDBs\n",
      "Group                                 \n",
      "G-protein            1141          706\n",
      "Beta-arrestin          58           19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CHAIN_CSV = 'Group/GPCR_PDB_chain.csv'\n",
    "GROUP_CSV = 'Group/GPCR_PDB_group.csv'\n",
    "\n",
    "chain_df = pd.read_csv(CHAIN_CSV)\n",
    "group_df = pd.read_csv(GROUP_CSV)\n",
    "\n",
    "passed_pdbs = set(chain_df.loc[chain_df['Pass'] == True, 'PDB'])\n",
    "\n",
    "valid_group_df = group_df[group_df['PDB'].isin(passed_pdbs)]\n",
    "\n",
    "total_counts = group_df['Group'].value_counts().rename(\"Total_PDBs\")\n",
    "valid_counts = valid_group_df['Group'].value_counts().rename(\"Passed_PDBs\")\n",
    "\n",
    "summary_df = pd.concat([total_counts, valid_counts], axis=1).fillna(0).astype(int)\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eee5f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 체인을 가진 PDB 수: 31\n",
      "------------------------------\n",
      "       PDB chain_id UniprotID  Pass\n",
      "5094  7E9G        R    Q14416  True\n",
      "5095  7E9G        S    Q14416  True\n",
      "3040  7E9H        R    Q14833  True\n",
      "3041  7E9H        S    Q14833  True\n",
      "2392  7MTS        A    Q14416  True\n",
      "...    ...      ...       ...   ...\n",
      "989   9M8V        E    P46089  True\n",
      "649   9MB9        D    O00222  True\n",
      "650   9MB9        R    O00222  True\n",
      "3000  9MBA        D    O00222  True\n",
      "3001  9MBA        R    O00222  True\n",
      "\n",
      "[62 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "csv_path = Path(\"/home/kjw/GPCRbias/Data/GPCR/Group/GPCR_PDB_chain.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 1. Pass가 True인 데이터만 필터링\n",
    "pass_df = df[df['Pass'] == True].copy()\n",
    "\n",
    "# 2. PDB별 고유 chain_id 개수 카운트\n",
    "chain_counts = pass_df.groupby('PDB')['chain_id'].nunique()\n",
    "\n",
    "# 3. chain이 2개 이상인 PDB 식별\n",
    "multi_chain_pdbs = chain_counts[chain_counts > 1].index\n",
    "\n",
    "# 4. 해당 PDB들의 상세 정보 출력\n",
    "result = pass_df[pass_df['PDB'].isin(multi_chain_pdbs)].sort_values(['PDB', 'chain_id'])\n",
    "\n",
    "print(f\"중복 체인을 가진 PDB 수: {len(multi_chain_pdbs)}\")\n",
    "print(\"-\" * 30)\n",
    "print(result[['PDB', 'chain_id', 'UniprotID', 'Pass']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6d7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hda2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
